{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "erPJoS32u7-v"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir(r'C:/Users/hakan/Jupyter Notebook/notebook/AIFarming/DataAnalysis/')\n",
    "# os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7DuNIKuBtz-f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_path ='C:/Users/hakan/Jupyter Notebook/data/porto-seguro-safe-driver-prediction/'\n",
    "train = pd.read_csv(data_path+'train.csv',index_col='id')\n",
    "test = pd.read_csv(data_path+'test.csv',index_col='id')\n",
    "submission = pd.read_csv(data_path+'sample_submission.csv',index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mxKubZxGxSnA"
   },
   "outputs": [],
   "source": [
    "def resumatable(df):\n",
    "    print(f'DataFrame Shape: {df.shape}')\n",
    "    summary = pd.DataFrame(df.dtypes, columns=['Dtype'])\n",
    "    summary['Missing'] = (df == -1).sum().values\n",
    "    summary['Unique'] = df.nunique().values\n",
    "    summary['Dtype'] = None\n",
    "    for col in df.columns:\n",
    "        if 'bin' in col or col =='target':\n",
    "            summary.loc[col,'Dtype'] = 'Binary'\n",
    "        elif 'cat' in col:\n",
    "            summary.loc[col,'Dtype'] = 'Categorical'\n",
    "        elif df[col].dtype == 'float64':\n",
    "            summary.loc[col,'Dtype'] = 'Continuous'\n",
    "        elif df[col].dtype == 'int64':\n",
    "            summary.loc[col,'Dtype'] = 'Ordinal'\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "saZDmxBu-6cs"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2FcBMKlcrXg3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_path ='C:/Users/hakan/Jupyter Notebook/data/porto-seguro-safe-driver-prediction/'\n",
    "train = pd.read_csv(data_path+'train.csv',index_col='id')\n",
    "test = pd.read_csv(data_path+'test.csv',index_col='id')\n",
    "submission = pd.read_csv(data_path+'sample_submission.csv',index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1731858830432,
     "user": {
      "displayName": "하강혁",
      "userId": "02638335608103264015"
     },
     "user_tz": -540
    },
    "id": "5PozTVy_wDWd",
    "outputId": "5dc0f464-96e2-4915-9436-db953bd37e26"
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([train,test],ignore_index=True)\n",
    "all_data = all_data.drop('target',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n",
       "       'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n",
       "       'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n",
       "       'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n",
       "       'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n",
       "       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n",
       "       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n",
       "       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n",
       "       'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n",
       "       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n",
       "       'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n",
       "       'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n",
       "       'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n",
       "       'ps_calc_20_bin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = all_data.columns\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#one hot encoding for categorical datas\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "onehot_encoder = OneHotEncoder()\n",
    "\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = ['ps_ind_10_bin','ps_ind_11_bin','ps_ind_12_bin','ps_ind_13_bin','ps_ind_14','ps_car_14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_features = [feature for feature in all_features if ('cat' not in feature and 'calc' not in feature and feature not in drop_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_01',\n",
       " 'ps_ind_03',\n",
       " 'ps_ind_06_bin',\n",
       " 'ps_ind_07_bin',\n",
       " 'ps_ind_08_bin',\n",
       " 'ps_ind_09_bin',\n",
       " 'ps_ind_15',\n",
       " 'ps_ind_16_bin',\n",
       " 'ps_ind_17_bin',\n",
       " 'ps_ind_18_bin',\n",
       " 'ps_reg_01',\n",
       " 'ps_reg_02',\n",
       " 'ps_reg_03',\n",
       " 'ps_car_11',\n",
       " 'ps_car_12',\n",
       " 'ps_car_13',\n",
       " 'ps_car_15']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse한 (ex : Onehot encoded) matrix를 다룰 때 scipy.sparse를 사용한다. 대규모 행렬을 다룰 때 메모리 문제를 해결하기 위해 사용한다. scipy.sparse의 matrix를 만드는 방법은 sparse matrix 구성요소를 직접 입력하는 방법과 numpy.ndarray를 입력하는 방법이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import  sparse\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data[remaining_features]),encoded_cat_matrix],format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1488028, 201), (595212, 58), (892816, 57))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_sprs.shape, train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595212\n"
     ]
    }
   ],
   "source": [
    "num_train = len(train)\n",
    "print(num_train)\n",
    "\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train:]\n",
    "\n",
    "y = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def eval_gini(y_true, y_pred):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    n_samples = y_true.shape[0]\n",
    "    L_mid = np.linspace(1/n_samples, 1, n_samples)\n",
    "\n",
    "    pred_order = y_true[y_pred.argsort()]\n",
    "    true_order = y_true[y_true.argsort()]\n",
    "    \n",
    "    L_pred = np.cumsum(pred_order)/np.sum(true_order)\n",
    "    G_pred = np.sum(L_mid - L_pred)\n",
    "    \n",
    "    \n",
    "    L_true = np.cumsum(true_order)/np.sum(true_order)\n",
    "    G_true = np.sum(L_mid - L_true)\n",
    "\n",
    "    #Normalized Gini coefficient\n",
    "    return G_pred/G_true    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gini for LGBM\n",
    "def gini(preds,dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', eval_gini(labels,preds), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object _BaseKFold.split at 0x000001C22A04A350>\n"
     ]
    }
   ],
   "source": [
    "print(folds.split(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective':'binary',\n",
    "    'learning_rate':0.01,\n",
    "    'force_row_wise':True,\n",
    "    'random_state':0,\n",
    "    'scale_pos_weight': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "oof_test_preds = np.zeros(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1095\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's binary_logloss: 0.155301\tvalid_0's gini: 0.243445\n",
      "fold 1 gini score : 0.2434451362266238\n",
      "\n",
      "######################################## 폴드 2 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1093\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's binary_logloss: 0.155345\tvalid_0's gini: 0.238196\n",
      "fold 2 gini score : 0.23819567157653104\n",
      "\n",
      "######################################## 폴드 3 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1097\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.155237\tvalid_0's gini: 0.247487\n",
      "fold 3 gini score : 0.2474874813186045\n",
      "\n",
      "######################################## 폴드 4 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1096\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.155311\tvalid_0's gini: 0.234866\n",
      "fold 4 gini score : 0.23486587102873827\n",
      "\n",
      "######################################## 폴드 5 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.155346\tvalid_0's gini: 0.246322\n",
      "fold 5 gini score : 0.24632175648589352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "#Normal Test\n",
    "for idx, (train_idx,valid_idx) in enumerate(folds.split(X,y)):\n",
    "    print('#'*40,f'폴드 {idx+1} / {folds.n_splits}','#'*40)\n",
    "    #Train, Validation data\n",
    "    X_train, y_train = X[train_idx],y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx],y[valid_idx]\n",
    "\n",
    "    #data set for lgbm\n",
    "    lgb_train = lgb.Dataset(X_train,y_train)\n",
    "    lgb_valid = lgb.Dataset(X_valid,y_valid)\n",
    "\n",
    "    # train lgbm model\n",
    "    lgb_model = lgb.train(params=lgb_params,\n",
    "                         train_set = lgb_train,\n",
    "                         num_boost_round=1000,\n",
    "                         valid_sets=lgb_valid,\n",
    "                         feval = gini,                                                   \n",
    "                         callbacks=[lgb.early_stopping(stopping_rounds=5)])\n",
    "\n",
    "    #oof prediction based on test data\n",
    "    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n",
    "    #based in validation data\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "\n",
    "    #gini\n",
    "    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'fold {idx+1} gini score : {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1095\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's binary_logloss: 0.155301\tvalid_0's gini: 0.243445\n",
      "fold 1 gini score : 0.2434451362266238\n",
      "\n",
      "######################################## 폴드 2 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1093\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's binary_logloss: 0.155345\tvalid_0's gini: 0.238196\n",
      "fold 2 gini score : 0.23819567157653104\n",
      "\n",
      "######################################## 폴드 3 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1097\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.155237\tvalid_0's gini: 0.247487\n",
      "fold 3 gini score : 0.2474874813186045\n",
      "\n",
      "######################################## 폴드 4 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1096\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.155311\tvalid_0's gini: 0.234866\n",
      "fold 4 gini score : 0.23486587102873827\n",
      "\n",
      "######################################## 폴드 5 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.155346\tvalid_0's gini: 0.246322\n",
      "fold 5 gini score : 0.24632175648589352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#is_unbalance\n",
    "for idx, (train_idx,valid_idx) in enumerate(folds.split(X,y)):\n",
    "    print('#'*40,f'폴드 {idx+1} / {folds.n_splits}','#'*40)\n",
    "    #Train, Validation data\n",
    "    X_train, y_train = X[train_idx],y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx],y[valid_idx]\n",
    "\n",
    "    #data set for lgbm\n",
    "    lgb_train = lgb.Dataset(X_train,y_train)\n",
    "    lgb_valid = lgb.Dataset(X_valid,y_valid)\n",
    "\n",
    "    # train lgbm model\n",
    "    lgb_model = lgb.train(params=lgb_params,\n",
    "                         train_set = lgb_train,\n",
    "                         num_boost_round=1000,\n",
    "                         valid_sets=lgb_valid,\n",
    "                         feval = gini,                            \n",
    "                         callbacks=[lgb.early_stopping(stopping_rounds=5)])\n",
    "\n",
    "    #oof prediction based on test data\n",
    "    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n",
    "    #based in validation data\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "\n",
    "    #gini\n",
    "    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'fold {idx+1} gini score : {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1095\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.156172\tvalid_0's gini: 0.22999\n",
      "fold 1 gini score : 0.24268400803554507\n",
      "\n",
      "######################################## 폴드 2 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1093\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.156199\tvalid_0's gini: 0.219243\n",
      "fold 2 gini score : 0.23727706318313563\n",
      "\n",
      "######################################## 폴드 3 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1097\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.156142\tvalid_0's gini: 0.235148\n",
      "fold 3 gini score : 0.24678607888029777\n",
      "\n",
      "######################################## 폴드 4 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1096\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.156186\tvalid_0's gini: 0.223763\n",
      "fold 4 gini score : 0.234347739808571\n",
      "\n",
      "######################################## 폴드 5 / 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.156196\tvalid_0's gini: 0.229913\n",
      "fold 5 gini score : 0.24545886884672297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    'objective':'binary',\n",
    "    'learning_rate':0.01,\n",
    "    'force_row_wise':True,\n",
    "    'random_state':0,\n",
    "    'scale_pos_weight': 10,\n",
    "}\n",
    "\n",
    "#scale_pos_weight :1.5~10 \n",
    "for idx, (train_idx,valid_idx) in enumerate(folds.split(X,y)):\n",
    "    print('#'*40,f'폴드 {idx+1} / {folds.n_splits}','#'*40)\n",
    "    #Train, Validation data\n",
    "    X_train, y_train = X[train_idx],y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx],y[valid_idx]\n",
    "\n",
    "    #data set for lgbm\n",
    "    lgb_train = lgb.Dataset(X_train,y_train)\n",
    "    lgb_valid = lgb.Dataset(X_valid,y_valid)\n",
    "\n",
    "    # train lgbm model\n",
    "    lgb_model = lgb.train(params=lgb_params,\n",
    "                         train_set = lgb_train,\n",
    "                         num_boost_round=1000,\n",
    "                         valid_sets=lgb_valid,\n",
    "                         feval = gini,                                                   \n",
    "                         callbacks=[lgb.early_stopping(stopping_rounds=5)])\n",
    "\n",
    "    #oof prediction based on test data\n",
    "    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n",
    "    #based in validation data\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "\n",
    "    #gini\n",
    "    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'fold {idx+1} gini score : {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24051405638858542\n"
     ]
    }
   ],
   "source": [
    "print(eval_gini(y,oof_val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.round(oof_test_preds))/len(oof_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('base_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6779991\ttotal: 166ms\tremaining: 2m 45s\n",
      "100:\tlearn: 0.1906039\ttotal: 2.85s\tremaining: 25.3s\n",
      "200:\tlearn: 0.1573332\ttotal: 5.55s\tremaining: 22.1s\n",
      "300:\tlearn: 0.1532955\ttotal: 8.3s\tremaining: 19.3s\n",
      "400:\tlearn: 0.1524549\ttotal: 11.1s\tremaining: 16.6s\n",
      "500:\tlearn: 0.1521069\ttotal: 13.9s\tremaining: 13.8s\n",
      "600:\tlearn: 0.1518821\ttotal: 16.7s\tremaining: 11.1s\n",
      "700:\tlearn: 0.1517073\ttotal: 19.5s\tremaining: 8.33s\n",
      "800:\tlearn: 0.1515623\ttotal: 22.4s\tremaining: 5.55s\n",
      "900:\tlearn: 0.1514323\ttotal: 25.2s\tremaining: 2.77s\n",
      "999:\tlearn: 0.1513168\ttotal: 28.1s\tremaining: 0us\n",
      "class :  [0 0 0 ... 0 0 0]\n",
      "proba :  [[0.97463692 0.02536308]\n",
      " [0.97460532 0.02539468]\n",
      " [0.97273679 0.02726321]\n",
      " ...\n",
      " [0.96242168 0.03757832]\n",
      " [0.97436631 0.02563369]\n",
      " [0.9689999  0.0310001 ]]\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# test_data = Pool(X,y)\n",
    "model = CatBoostClassifier(iterations =1000, depth=5,learning_rate = 0.01,loss_function='Logloss',verbose=100,od_pval=0.01)\n",
    "\n",
    "history = model.fit(X,y)\n",
    "preds_class = model.predict(X_test)\n",
    "preds_proba = model.predict_proba(X_test)\n",
    "print('class : ',preds_class)\n",
    "print('proba : ',preds_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97463692 0.97460532 0.97273679 ... 0.96242168 0.97436631 0.9689999 ]\n",
      "0.9635820950969949\n",
      "0.036417904903005484\n"
     ]
    }
   ],
   "source": [
    "print(preds_proba[:,0])\n",
    "np.round(preds_proba)\n",
    "print(np.sum(preds_proba[:,0])/preds_proba.shape[0])\n",
    "print(np.sum(preds_proba[:,1])/preds_proba.shape[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPKtZoQ1hQKOKFa2g5z/woE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
